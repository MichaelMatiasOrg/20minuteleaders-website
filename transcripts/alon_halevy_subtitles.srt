1
00:04:54,500 --> 00:04:56,120
Alon Halevy, thank you so much

2
00:04:56,120 --> 00:04:56,840
for being on my show.

3
00:04:56,840 --> 00:04:57,840
How are you?

4
00:04:57,840 --> 00:04:59,160
I'm doing great. How are you?

5
00:04:59,160 --> 00:05:01,400
How have you been during these crazy time?

6
00:05:01,400 --> 00:05:05,120
You know, it's been a difficult

7
00:05:05,120 --> 00:05:06,720
and very challenging time,

8
00:05:06,720 --> 00:05:08,840
but, I'm trying to make the most of it.

9
00:05:08,840 --> 00:05:12,640
So, I've learned how to cook

10
00:05:12,640 --> 00:05:14,600
out of sheer need.

11
00:05:14,600 --> 00:05:15,880
And my cooking skills

12
00:05:15,880 --> 00:05:17,720
have just skyrocketed to a point

13
00:05:17,720 --> 00:05:20,960
where you can actually eat my food.

14
00:05:20,960 --> 00:05:21,800
I've done gardening.

15
00:05:21,800 --> 00:05:22,800
I've been exercising.

16
00:05:22,800 --> 00:05:24,840
So, if you look, I'm actually,

17
00:05:24,840 --> 00:05:26,960
mentally in pretty good shape.

18
00:05:26,960 --> 00:05:28,080
That's pretty fantastic.

19
00:05:28,080 --> 00:05:29,280
I think, it's always fascinating for me

20
00:05:29,280 --> 00:05:30,400
to see how every person

21
00:05:30,400 --> 00:05:31,840
interacts differently.

22
00:05:31,840 --> 00:05:32,800
I was just talking about it

23
00:05:32,800 --> 00:05:34,440
with our family recently

24
00:05:34,440 --> 00:05:36,920
about how it's crazy how we all have this

25
00:05:36,920 --> 00:05:39,320
collective crisis and every person now

26
00:05:39,320 --> 00:05:40,240
manages it differently.

27
00:05:40,240 --> 00:05:42,120
But Alon, one thing that is different

28
00:05:42,120 --> 00:05:43,400
from your daily life is that

29
00:05:43,400 --> 00:05:44,960
you are a big traveler.

30
00:05:44,960 --> 00:05:45,880
And you actually traveled

31
00:05:45,880 --> 00:05:47,320
to 30 different countries,

32
00:05:47,320 --> 00:05:50,000
writing a book about coffee.

33
00:05:50,000 --> 00:05:52,360
Now, for context, you're also a director

34
00:05:52,360 --> 00:05:53,720
within Facebook AI.

35
00:05:53,720 --> 00:05:55,880
Yet, you go and you are writing a book

36
00:05:55,880 --> 00:05:58,120
about coffee. What is that all about?

37
00:05:58,120 --> 00:06:01,440
Well, so, good question.

38
00:06:01,440 --> 00:06:02,960
So, the book was actually written

39
00:06:02,960 --> 00:06:04,800
when I was working for Google.

40
00:06:04,800 --> 00:06:06,240
Okay.

41
00:06:06,240 --> 00:06:08,760
Google gets the credit for the book,

42
00:06:08,760 --> 00:06:10,360
Google gets the credit.

43
00:06:10,360 --> 00:06:16,320
And I think, I have no good explanation

44
00:06:16,320 --> 00:06:19,800
for why I decided at some point in my 40s

45
00:06:19,800 --> 00:06:22,280
that I need to write a book about coffee.

46
00:06:22,280 --> 00:06:24,080
I always like to go to cafes.

47
00:06:24,080 --> 00:06:26,160
I always like the vibe in cafes.

48
00:06:26,160 --> 00:06:28,520
And I always noticed that

49
00:06:28,520 --> 00:06:31,200
in different countries you go,

50
00:06:31,280 --> 00:06:34,000
you get different experiences in cafes.

51
00:06:34,000 --> 00:06:35,200
And so, I was very curious

52
00:06:35,200 --> 00:06:36,400
about trying to map that

53
00:06:36,400 --> 00:06:38,880
and studied it at a little more detail,

54
00:06:38,880 --> 00:06:39,880
why am I getting these

55
00:06:39,880 --> 00:06:41,440
different experiences.

56
00:06:41,440 --> 00:06:44,800
And then, I did something which ultimately

57
00:06:44,800 --> 00:06:45,680
led to writing the book.

58
00:06:45,680 --> 00:06:47,840
I started telling my friends that I think,

59
00:06:47,840 --> 00:06:49,720
I should write a book about coffee.

60
00:06:49,720 --> 00:06:51,160
And after a few months,

61
00:06:51,160 --> 00:06:52,880
my friends have start coming back to me

62
00:06:52,880 --> 00:06:55,720
and say, "Hey, where's the book?"

63
00:06:55,720 --> 00:06:58,680
And once you sign a social contract,

64
00:06:58,680 --> 00:07:02,000
you have no choice but to do it.

65
00:07:02,000 --> 00:07:04,240
Alon, I love it so much.

66
00:07:04,240 --> 00:07:05,480
I created a Google map.

67
00:07:05,480 --> 00:07:08,360
And I listed through three different

68
00:07:08,360 --> 00:07:09,800
countries that one must visit

69
00:07:09,800 --> 00:07:12,200
in order to write a book about coffee.

70
00:07:12,280 --> 00:07:14,200
Many of them I was travelling to,

71
00:07:14,200 --> 00:07:15,320
or to the area anyway,

72
00:07:15,360 --> 00:07:18,080
because of the nature of my job.

73
00:07:18,080 --> 00:07:19,240
Right.

74
00:07:19,240 --> 00:07:20,200
Some of them, I actually,

75
00:07:20,200 --> 00:07:22,480
had to take a few days off.

76
00:07:22,480 --> 00:07:23,240
That's hilarious.

77
00:07:23,240 --> 00:07:25,800
ALon, how many cups do you drink a day?

78
00:07:25,800 --> 00:07:26,600
Just for context.

79
00:07:26,600 --> 00:07:28,280
Only two. But they're very high quality.

80
00:07:28,280 --> 00:07:29,240
Only two.

81
00:07:29,240 --> 00:07:30,720
Only two.

82
00:07:30,720 --> 00:07:32,280
If I'm at a coffee conference,

83
00:07:32,280 --> 00:07:34,040
which A, these things exist;

84
00:07:34,040 --> 00:07:34,400
and B,

85
00:07:34,400 --> 00:07:36,040
The coffee conference.

86
00:07:36,040 --> 00:07:37,640
That must be one hell of an energy

87
00:07:37,640 --> 00:07:38,920
in that auditorium.

88
00:07:38,920 --> 00:07:39,400
Yes.

89
00:07:39,400 --> 00:07:41,680
And when I would drink somewhere

90
00:07:41,680 --> 00:07:47,000
between 10 and 15 espressos, or something.

91
00:07:47,000 --> 00:07:48,200
Okay. And before we move on

92
00:07:48,200 --> 00:07:49,640
to technology and high tech,

93
00:07:49,640 --> 00:07:51,000
what is your favorite coffee

94
00:07:51,000 --> 00:07:52,960
you've ever tried in your life?

95
00:07:52,960 --> 00:07:55,200
The country and place.

96
00:07:55,200 --> 00:07:59,040
The favorite is from a region

97
00:07:59,040 --> 00:08:03,080
called the Yirgacheffe in Ethiopia.

98
00:08:04,080 --> 00:08:08,840
It came from a one the cooperative there.

99
00:08:08,840 --> 00:08:10,720
It just blew my mind.

100
00:08:10,720 --> 00:08:13,920
That's a thing. Great coffee,

101
00:08:13,920 --> 00:08:15,440
will just blow your mind.

102
00:08:15,440 --> 00:08:17,400
One of the hardest transitions I've had

103
00:08:17,400 --> 00:08:18,960
coming from Tel Aviv to Palo Alto,

104
00:08:18,960 --> 00:08:21,280
I have to admit, is getting used to

105
00:08:21,280 --> 00:08:23,320
not having my afuch in the morning,

106
00:08:23,320 --> 00:08:24,000
and my cappuccino.

107
00:08:24,000 --> 00:08:25,520
So, that was definitely a big change.

108
00:08:25,520 --> 00:08:27,320
But Alon, we only have 15 minutes left.

109
00:08:27,320 --> 00:08:28,880
I have wasted way too much about coffee.

110
00:08:28,880 --> 00:08:30,600
Take me through your journey.

111
00:08:30,600 --> 00:08:31,880
You start out as an entrepreneur.

112
00:08:31,880 --> 00:08:33,080
Move on to researcher.

113
00:08:33,280 --> 00:08:35,760
Go through these incredible companies,

114
00:08:35,760 --> 00:08:38,440
like Google, now Facebook, Bell Labs.

115
00:08:38,440 --> 00:08:41,640
Why are you so curious

116
00:08:41,640 --> 00:08:43,160
about your domain expertise?

117
00:08:43,160 --> 00:08:44,080
You're also professor

118
00:08:44,080 --> 00:08:46,160
at University of Washington.

119
00:08:46,160 --> 00:08:48,000
Walk me through your journey a little bit.

120
00:08:48,000 --> 00:08:53,240
Okay. So, I had no doubt in life

121
00:08:53,280 --> 00:08:55,280
that I needed to be a professor.

122
00:08:55,280 --> 00:08:56,120
Really?

123
00:08:56,120 --> 00:08:58,000
My father was a professor.

124
00:08:58,000 --> 00:09:00,080
It looked like the right job to do.

125
00:09:00,080 --> 00:09:01,600
So, I never actually

126
00:09:01,600 --> 00:09:04,040
had a point in my life

127
00:09:04,040 --> 00:09:06,040
where I had a hard decision to make.

128
00:09:06,040 --> 00:09:06,760
Interesting.

129
00:09:06,760 --> 00:09:07,280
So, it's obvious

130
00:09:07,280 --> 00:09:08,640
that I wanted to do a PhD.

131
00:09:08,640 --> 00:09:10,840
At Stanford, of course.

132
00:09:10,840 --> 00:09:13,320
I came to Stanford, of course.

133
00:09:13,320 --> 00:09:15,160
And then, after I finished my PhD,

134
00:09:15,160 --> 00:09:17,040
nobody gave me a professor job.

135
00:09:17,040 --> 00:09:19,280
So, I went to, what was at the time

136
00:09:19,280 --> 00:09:20,760
a pretty prestigious lab

137
00:09:20,760 --> 00:09:23,520
called AT&T Bell Labs.

138
00:09:23,520 --> 00:09:25,520
And a few years later,

139
00:09:25,520 --> 00:09:26,800
I got an offer to be a professor

140
00:09:26,800 --> 00:09:28,640
at the University of Washington.

141
00:09:28,640 --> 00:09:29,760
Wow.

142
00:09:29,760 --> 00:09:30,920
Then I realized that,

143
00:09:30,920 --> 00:09:32,640
this is within the height of the 90s

144
00:09:32,720 --> 00:09:36,240
when all these startups were going on.

145
00:09:36,240 --> 00:09:37,840
Is this where you actually met my father

146
00:09:37,880 --> 00:09:38,980
back in Bell Labs?

147
00:09:38,980 --> 00:09:40,400
In Bell Labs, yes.

148
00:09:40,400 --> 00:09:41,360
Okay. Okay.

149
00:09:41,360 --> 00:09:42,530
Now, I'm making the connection.

150
00:09:42,530 --> 00:09:43,200
Yeah. Yeah.

151
00:09:43,200 --> 00:09:45,080
Your father and I, were both part

152
00:09:45,080 --> 00:09:46,880
of the database community,

153
00:09:46,880 --> 00:09:48,040
the academic database.

154
00:09:48,040 --> 00:09:50,920
So, I've known him since I graduated,

155
00:09:50,920 --> 00:09:52,040
maybe even before.

156
00:09:52,040 --> 00:09:53,880
Wonderful.

157
00:09:53,880 --> 00:09:55,280
So, first of all, did that professorship

158
00:09:55,280 --> 00:09:56,520
live up to your expectations?

159
00:09:56,520 --> 00:09:57,840
Why did you want to be a professor?

160
00:09:57,840 --> 00:09:58,400
I mean, obviously, yeah,

161
00:09:58,400 --> 00:09:59,480
your father was a professor,

162
00:09:59,480 --> 00:10:02,240
but what attracted you to academia so much

163
00:10:02,240 --> 00:10:05,240
as you're seeing the dot-com rise,

164
00:10:05,240 --> 00:10:06,360
and obviously with the bubble,

165
00:10:06,360 --> 00:10:08,240
but everything rise?

166
00:10:08,240 --> 00:10:13,520
I think, at my very core,

167
00:10:13,520 --> 00:10:15,520
I like to think about new ideas,

168
00:10:15,520 --> 00:10:17,080
break new ground,

169
00:10:17,200 --> 00:10:21,280
and ask the questions that will lead

170
00:10:21,280 --> 00:10:23,760
other researchers and engineers

171
00:10:23,760 --> 00:10:25,600
and companies to think about

172
00:10:25,600 --> 00:10:28,040
the next generation of products.

173
00:10:28,040 --> 00:10:31,840
So, to do that, being a professor

174
00:10:31,840 --> 00:10:35,080
is a good place to do that.

175
00:10:35,080 --> 00:10:36,920
I really enjoyed working with students.

176
00:10:36,920 --> 00:10:38,720
I had some pretty awesome students

177
00:10:38,720 --> 00:10:42,200
at the University of Washington.

178
00:10:42,200 --> 00:10:45,480
And I would still recommended it.

179
00:10:45,480 --> 00:10:47,720
I mean, being a professor is

180
00:10:47,720 --> 00:10:48,880
what you can do must worth.

181
00:10:48,880 --> 00:10:53,560
I also have a entrepreneurial bone

182
00:10:53,560 --> 00:10:54,360
in my body.

183
00:10:54,360 --> 00:10:55,280
Yeah. So, take me through

184
00:10:55,280 --> 00:10:56,200
your two companies.

185
00:10:56,200 --> 00:10:57,960
The second one was acquired by Google,

186
00:10:57,960 --> 00:10:58,440
I believe. Right?

187
00:10:58,440 --> 00:11:00,240
That's when you joined Google?

188
00:11:00,240 --> 00:11:02,000
Yes. So, the second company

189
00:11:02,000 --> 00:11:04,360
was based on a project that we did

190
00:11:04,360 --> 00:11:06,840
at the University of Washington.

191
00:11:06,840 --> 00:11:08,560
We developed some technology

192
00:11:08,560 --> 00:11:10,360
that enabled search engines

193
00:11:10,360 --> 00:11:13,120
to actually crawl beyond forms

194
00:11:13,120 --> 00:11:14,080
on web pages.

195
00:11:14,080 --> 00:11:16,480
So, if you saw a form until then,

196
00:11:16,480 --> 00:11:17,400
you needed a human

197
00:11:17,400 --> 00:11:19,120
to actually fill in fields of the form

198
00:11:19,120 --> 00:11:20,560
and to get all the content.

199
00:11:20,560 --> 00:11:21,320
We developed a method

200
00:11:21,320 --> 00:11:26,440
that was able to do that automatically,

201
00:11:26,440 --> 00:11:28,600
and therefore, add many more

202
00:11:28,600 --> 00:11:31,200
high quality pages

203
00:11:31,200 --> 00:11:33,800
to the Google index at the time.

204
00:11:33,800 --> 00:11:35,200
And we're actually still looking back

205
00:11:35,200 --> 00:11:37,600
at 2005, 2006. Right?

206
00:11:37,600 --> 00:11:40,040
These are around the time.

207
00:11:40,040 --> 00:11:41,560
So, it's still very early.

208
00:11:41,560 --> 00:11:44,200
And very few people

209
00:11:44,200 --> 00:11:46,040
talked about web development

210
00:11:46,040 --> 00:11:48,480
and websites just became a thing.

211
00:11:49,480 --> 00:11:51,440
That is true. That is true.

212
00:11:51,440 --> 00:11:52,840
It's now that I think back at it,

213
00:11:52,840 --> 00:11:55,400
that's a good point.

214
00:11:55,400 --> 00:11:59,240
We still didn't understand,

215
00:11:59,240 --> 00:12:00,160
web search were still,

216
00:12:00,160 --> 00:12:01,040
even though Google was already

217
00:12:01,040 --> 00:12:02,080
a public company when I joined it,

218
00:12:02,080 --> 00:12:06,160
web search was still in its infancy.

219
00:12:06,160 --> 00:12:07,320
And it was a really,

220
00:12:07,400 --> 00:12:09,400
I remember at the time, Google was

221
00:12:09,400 --> 00:12:11,480
an extremely exciting place to be at.

222
00:12:11,480 --> 00:12:14,440
I have a group of people who are coming,

223
00:12:14,440 --> 00:12:16,240
I just had a very fascinating chat

224
00:12:16,240 --> 00:12:20,760
with Marcel Kornacker from Apache Impala.

225
00:12:20,760 --> 00:12:22,400
And he mentioned that he was at Google

226
00:12:22,400 --> 00:12:24,160
also in around 2003, 2004,

227
00:12:24,160 --> 00:12:25,160
working on databases,

228
00:12:25,160 --> 00:12:26,800
and how exciting it was at the time

229
00:12:26,800 --> 00:12:28,000
when it was just people

230
00:12:28,000 --> 00:12:30,040
are understanding what's the potential.

231
00:12:30,040 --> 00:12:31,280
Along, give me your two cents quickly

232
00:12:31,280 --> 00:12:33,800
about this intersection between

233
00:12:33,800 --> 00:12:35,200
academia and startups.

234
00:12:35,200 --> 00:12:36,120
You're working on a research

235
00:12:36,120 --> 00:12:37,000
as a part of academia

236
00:12:37,000 --> 00:12:38,560
which naturally flows

237
00:12:38,600 --> 00:12:40,200
into the high tech scene.

238
00:12:40,200 --> 00:12:41,640
You've formed a company out of your work

239
00:12:41,640 --> 00:12:43,080
in the University of Washington.

240
00:12:43,080 --> 00:12:44,920
Stanford professors are very well known

241
00:12:44,920 --> 00:12:46,080
to do this type of thing.

242
00:12:46,080 --> 00:12:48,160
Andrew Ng, I believe, runs about 10 or 11

243
00:12:48,160 --> 00:12:49,720
different startups at the same time.

244
00:12:49,720 --> 00:12:51,920
So, what is this?

245
00:12:51,920 --> 00:12:53,080
Is there a dissonance?

246
00:12:53,080 --> 00:12:55,120
Do they want you to do this?

247
00:12:55,120 --> 00:12:56,240
Is this [--]

248
00:12:56,240 --> 00:13:00,840
So, that is something that has,

249
00:13:00,840 --> 00:13:01,960
over the years,

250
00:13:01,960 --> 00:13:05,800
the universities realize that

251
00:13:05,800 --> 00:13:07,880
this is a good thing for them, right?

252
00:13:07,880 --> 00:13:11,560
So, if you go back even 20, 25 years,

253
00:13:11,560 --> 00:13:13,120
it was a difficult thing to do.

254
00:13:13,120 --> 00:13:14,960
Universities like Stanford and MIT,

255
00:13:14,960 --> 00:13:17,520
realized early on that

256
00:13:17,520 --> 00:13:20,440
being part of that ecosystem

257
00:13:20,440 --> 00:13:23,200
is an important thing for the industry

258
00:13:23,200 --> 00:13:25,240
to be involved in.

259
00:13:25,240 --> 00:13:26,960
And then you get a bunch of lawyers

260
00:13:26,960 --> 00:13:27,520
in the room, right?

261
00:13:27,520 --> 00:13:29,040
And so, it takes a while

262
00:13:29,040 --> 00:13:30,280
to work out the process

263
00:13:30,320 --> 00:13:32,080
and to make sure that everybody

264
00:13:32,080 --> 00:13:33,880
is, has right credit.

265
00:13:33,880 --> 00:13:34,960
The university gets a cut,

266
00:13:34,960 --> 00:13:36,680
the students that were involved get a cut.

267
00:13:36,680 --> 00:13:38,900
Whatever. All these things worked out.

268
00:13:38,900 --> 00:13:43,920
And also, universities were letting

269
00:13:43,920 --> 00:13:46,640
faculty leave for a year or two

270
00:13:46,640 --> 00:13:47,600
to work on their company

271
00:13:47,600 --> 00:13:49,200
and then come back.

272
00:13:49,200 --> 00:13:49,720
Which is really important.

273
00:13:49,720 --> 00:13:50,600
Because it's very hard

274
00:13:50,600 --> 00:13:53,320
to start a successful company

275
00:13:53,320 --> 00:13:55,600
and not be full time at the company,

276
00:13:55,600 --> 00:13:58,400
especially if you're the founder.

277
00:13:58,400 --> 00:14:02,360
So, I think today is very different

278
00:14:02,360 --> 00:14:04,040
than what it was when,

279
00:14:04,040 --> 00:14:06,120
like 15 or 20 years ago.

280
00:14:06,120 --> 00:14:08,640
I think today, it's taken for granted

281
00:14:08,640 --> 00:14:11,280
that people are going to do this.

282
00:14:11,280 --> 00:14:12,200
And I think, the process

283
00:14:12,200 --> 00:14:14,240
is much more well established.

284
00:14:14,240 --> 00:14:16,440
But again, it varies by university

285
00:14:16,440 --> 00:14:18,120
and country.

286
00:14:18,120 --> 00:14:20,840
And it's not easy everywhere.

287
00:14:20,840 --> 00:14:21,360
All right. Okay.

288
00:14:21,360 --> 00:14:23,480
So, I'd like to fast forward

289
00:14:23,480 --> 00:14:24,840
to present day.

290
00:14:24,840 --> 00:14:27,240
You're a director within

291
00:14:27,240 --> 00:14:28,800
the Facebook AI group.

292
00:14:28,800 --> 00:14:30,720
And everybody talks about AI.

293
00:14:30,720 --> 00:14:32,280
And especially now as we're approaching,

294
00:14:32,280 --> 00:14:33,720
2020 is a monumental year.

295
00:14:33,720 --> 00:14:35,080
And not just because covid,

296
00:14:35,080 --> 00:14:37,480
which is a very data driven pandemic.

297
00:14:37,480 --> 00:14:38,280
I mean, the whole world

298
00:14:38,280 --> 00:14:39,120
is trying to figure out

299
00:14:39,160 --> 00:14:40,480
what the hell is happening.

300
00:14:40,480 --> 00:14:42,000
But also we have elections coming up

301
00:14:42,000 --> 00:14:42,360
and you have,

302
00:14:42,440 --> 00:14:44,640
data is just playing such a more

303
00:14:44,640 --> 00:14:46,480
significant role in our lives.

304
00:14:46,480 --> 00:14:47,120
More and more.

305
00:14:47,120 --> 00:14:47,560
And that's something

306
00:14:47,560 --> 00:14:48,480
that I'm very curious about

307
00:14:48,480 --> 00:14:49,280
within Stanford.

308
00:14:49,280 --> 00:14:50,880
At the level of a director,

309
00:14:50,880 --> 00:14:52,560
with your experience in the field

310
00:14:52,560 --> 00:14:53,640
and with the responsibilities

311
00:14:53,640 --> 00:14:54,640
that you hold,

312
00:14:54,640 --> 00:14:55,800
talk to me a little bit about

313
00:14:55,800 --> 00:14:57,360
why you're passionate about

314
00:14:57,360 --> 00:14:59,480
the field of artificial intelligence.

315
00:14:59,480 --> 00:15:01,560
And maybe, a little bit about

316
00:15:01,560 --> 00:15:03,840
our responsibility as we develop

317
00:15:03,840 --> 00:15:07,720
responsible and ethical AI.

318
00:15:07,720 --> 00:15:08,920
Wonderful topic.

319
00:15:08,920 --> 00:15:11,920
I'm totally into that right now.

320
00:15:11,920 --> 00:15:15,760
So, the short answer is

321
00:15:15,760 --> 00:15:17,560
there's never been a time in history

322
00:15:17,560 --> 00:15:21,920
where AI was so crucial to the continuing

323
00:15:21,920 --> 00:15:25,000
functional behavior of humanity.

324
00:15:25,000 --> 00:15:25,760
Okay?

325
00:15:25,760 --> 00:15:28,280
So, let me be very specific.

326
00:15:28,280 --> 00:15:29,760
Social networks are awesome, right?

327
00:15:29,760 --> 00:15:30,800
They enable anybody anybody

328
00:15:30,800 --> 00:15:31,800
to have a voice, right?

329
00:15:31,800 --> 00:15:34,040
They enable, whether it's a famous person

330
00:15:34,040 --> 00:15:34,960
or your next door neighbor.

331
00:15:34,960 --> 00:15:35,960
Anybody can say anything.

332
00:15:35,960 --> 00:15:37,640
You can start connecting with people

333
00:15:37,640 --> 00:15:38,880
in other parts of the world

334
00:15:38,920 --> 00:15:40,040
that share common interests

335
00:15:40,040 --> 00:15:42,240
that you never met before.

336
00:15:42,240 --> 00:15:49,000
Wonderful social value

337
00:15:49,000 --> 00:15:49,480
that you network with.

338
00:15:49,480 --> 00:15:51,200
No argument there. Yeah.

339
00:15:51,200 --> 00:15:52,600
But, and there's always a but

340
00:15:52,600 --> 00:15:54,840
when there's a great technology, right?

341
00:15:54,840 --> 00:16:01,120
Social networks also are a window

342
00:16:01,120 --> 00:16:03,320
into the worst in humanity.

343
00:16:03,320 --> 00:16:05,640
Whether it is people

344
00:16:05,640 --> 00:16:07,760
putting up hate speech.

345
00:16:07,760 --> 00:16:09,440
People creating

346
00:16:09,440 --> 00:16:12,280
and disseminating misinformation.

347
00:16:12,280 --> 00:16:14,120
People exploiting other people

348
00:16:14,120 --> 00:16:21,680
for various, there are things

349
00:16:21,680 --> 00:16:23,480
that I don't even have to mention.

350
00:16:23,480 --> 00:16:25,960
People try and sell illegal items

351
00:16:25,960 --> 00:16:28,320
on social networks.

352
00:16:28,320 --> 00:16:29,280
So, the number,

353
00:16:29,280 --> 00:16:30,400
and we call these violations,

354
00:16:30,440 --> 00:16:35,120
these are policy violations.

355
00:16:35,120 --> 00:16:36,960
But I think, the big thing here

356
00:16:36,960 --> 00:16:38,480
is that we're talking about scale.

357
00:16:38,480 --> 00:16:40,920
So, you can have a thousand people

358
00:16:40,920 --> 00:16:42,960
sitting around and reading every post,

359
00:16:42,960 --> 00:16:44,720
but when you get to millions

360
00:16:44,720 --> 00:16:48,040
or billions of posts a day, at that scale,

361
00:16:48,040 --> 00:16:50,280
it becomes a feat, right?

362
00:16:50,280 --> 00:16:52,880
Yes. And so, this problem

363
00:16:52,880 --> 00:16:54,080
is known as the problem

364
00:16:54,400 --> 00:16:57,560
of preserving integrity with networks.

365
00:16:57,560 --> 00:16:57,840
Okay?

366
00:16:57,840 --> 00:16:59,960
And hopefully, in a few weeks from now,

367
00:16:59,960 --> 00:17:00,760
there will be a survey

368
00:17:00,760 --> 00:17:05,200
that a bunch colleagues and I, at Facebook

369
00:17:05,200 --> 00:17:06,920
will have finished writing

370
00:17:06,920 --> 00:17:08,520
about this topic that I recommend to them.

371
00:17:08,520 --> 00:17:09,520
So, we're releasing this video

372
00:17:09,520 --> 00:17:10,800
after that survey is published

373
00:17:10,800 --> 00:17:11,840
so that I can put the link there.

374
00:17:11,880 --> 00:17:14,080
because I'm sure, it'll be fascinating.

375
00:17:14,080 --> 00:17:14,680
Sounds good.

376
00:17:14,680 --> 00:17:15,680
Though, this might take a while

377
00:17:15,680 --> 00:17:17,320
because it needs to be released.

378
00:17:17,320 --> 00:17:17,960
Oh, we have time.

379
00:17:18,000 --> 00:17:19,320
Covid Is here to stay for a long time.

380
00:17:19,320 --> 00:17:21,400
Don't worry. This show is gonna go on.

381
00:17:21,400 --> 00:17:22,280
Okay. Good.

382
00:17:22,280 --> 00:17:27,920
So, it is crucial to social networks

383
00:17:28,200 --> 00:17:31,320
to keep the content on the network

384
00:17:31,320 --> 00:17:33,680
safe for the users, okay?

385
00:17:33,680 --> 00:17:35,680
And as you pointed out,

386
00:17:35,680 --> 00:17:37,760
doing this manually is impossible.

387
00:17:37,760 --> 00:17:38,600
Right.

388
00:17:38,600 --> 00:17:39,560
So, what we have is,

389
00:17:39,560 --> 00:17:41,760
we have a bunch of AIs systems

390
00:17:41,760 --> 00:17:43,840
that are constantly looking

391
00:17:43,840 --> 00:17:46,640
at content coming in. Okay?

392
00:17:46,640 --> 00:17:49,280
When these AI systems

393
00:17:49,280 --> 00:17:50,320
are extremely confident

394
00:17:50,320 --> 00:17:52,920
that this content is violating,

395
00:17:52,920 --> 00:17:54,160
they will remove it automatically.

396
00:17:54,160 --> 00:17:55,760
When they are not confident,

397
00:17:55,760 --> 00:17:58,680
or when they will receive a user flag,

398
00:17:58,680 --> 00:18:00,440
so, the users will see content and say,

399
00:18:00,440 --> 00:18:01,680
"Hey, this is what you should do."

400
00:18:01,680 --> 00:18:05,680
We will send it to a large number

401
00:18:05,680 --> 00:18:09,040
of paid workers who will look at it

402
00:18:09,040 --> 00:18:11,560
and see whether it violates our policies.

403
00:18:11,560 --> 00:18:13,520
By the way, in terms of the ripeness

404
00:18:13,520 --> 00:18:14,560
of the technology and of course,

405
00:18:14,560 --> 00:18:15,400
if I ever ask something

406
00:18:15,600 --> 00:18:16,760
that is not suitable for you,

407
00:18:16,760 --> 00:18:17,480
let me know.

408
00:18:17,480 --> 00:18:18,640
But in terms of ripeness

409
00:18:18,680 --> 00:18:19,480
of the technology,

410
00:18:19,680 --> 00:18:22,000
where are we at now?

411
00:18:22,000 --> 00:18:24,560
Is it more so on the automatic flagging?

412
00:18:24,560 --> 00:18:31,160
Or, more so on the on the user flagging?

413
00:18:31,160 --> 00:18:33,880
So, Facebook actually publishes

414
00:18:33,880 --> 00:18:37,480
every six months a report on the numbers.

415
00:18:37,480 --> 00:18:38,720
Okay. Okay.

416
00:18:38,720 --> 00:18:41,400
So, actually some of these,

417
00:18:41,400 --> 00:18:42,800
and there are like tens

418
00:18:42,800 --> 00:18:45,280
of different violations.

419
00:18:45,280 --> 00:18:47,840
I should say, coming up with policies

420
00:18:47,840 --> 00:18:48,920
for what is allowed

421
00:18:48,920 --> 00:18:50,160
and what is not allowed on social network,

422
00:18:50,160 --> 00:18:52,880
That by itself is,

423
00:18:52,880 --> 00:18:53,840
I'm not going to talk about that

424
00:18:53,840 --> 00:18:54,960
because that's going to get me

425
00:18:54,960 --> 00:18:56,760
on a rathole

426
00:18:56,760 --> 00:18:59,240
that I am not authorized to go down, okay?

427
00:18:59,240 --> 00:19:00,200
But you can imagine, right?

428
00:19:00,200 --> 00:19:01,280
This is a really really hard part.

429
00:19:01,280 --> 00:19:05,320
It's based on culture.

430
00:19:05,320 --> 00:19:07,040
Yeah. That's exactly the problem.

431
00:19:07,040 --> 00:19:08,440
The classic problem. Yeah.

432
00:19:08,440 --> 00:19:10,800
Yes. And because, fundamentally,

433
00:19:10,800 --> 00:19:11,480
what you're trying to do

434
00:19:11,480 --> 00:19:12,560
is you're trying to balance

435
00:19:12,560 --> 00:19:14,800
between safety of your users

436
00:19:14,800 --> 00:19:16,720
and giving people a voice.

437
00:19:16,720 --> 00:19:17,800
Yes. Yes.

438
00:19:17,800 --> 00:19:19,960
Very small nuances.

439
00:19:19,960 --> 00:19:22,040
For example, you are not allowed

440
00:19:22,040 --> 00:19:26,720
to advertise, to sell guns on Facebook.

441
00:19:26,720 --> 00:19:28,160
That's part of the policy.

442
00:19:28,160 --> 00:19:31,000
But if you're a gun retailer,

443
00:19:31,200 --> 00:19:32,320
you're allowed to talk

444
00:19:32,320 --> 00:19:34,640
about what's available in your store.

445
00:19:34,640 --> 00:19:36,760
It's, those nuances are crazy.

446
00:19:36,760 --> 00:19:38,320
But Alon, I do want to save some time also

447
00:19:38,320 --> 00:19:39,560
for thinking a little bit broader

448
00:19:39,560 --> 00:19:40,840
about the world of AI

449
00:19:40,840 --> 00:19:42,360
and where we're headed as a society.

450
00:19:42,360 --> 00:19:43,880
So, yes, you've got to see

451
00:19:43,880 --> 00:19:45,640
both Google and Facebook.

452
00:19:45,640 --> 00:19:47,200
And while that's your domain expertise now,

453
00:19:47,240 --> 00:19:49,120
I want to extrapolate with you

454
00:19:49,120 --> 00:19:52,080
a little bit about,  what are my kids

455
00:19:52,080 --> 00:19:53,680
going to experience in terms

456
00:19:53,680 --> 00:19:55,200
of their online behavior?

457
00:19:55,200 --> 00:19:57,400
Where do you see AI integrating

458
00:19:57,400 --> 00:19:59,120
into our online environment

459
00:19:59,120 --> 00:20:00,400
as the years develop?

460
00:20:00,400 --> 00:20:02,480
Because we'll get over our current comps,

461
00:20:02,480 --> 00:20:03,640
we'll get over the elections

462
00:20:03,640 --> 00:20:04,920
and covid and everything.

463
00:20:04,920 --> 00:20:06,200
I want to know

464
00:20:06,200 --> 00:20:07,560
what my kids going to expect

465
00:20:07,560 --> 00:20:10,600
in terms of their integrations.

466
00:20:10,600 --> 00:20:12,360
That's a great question.

467
00:20:12,360 --> 00:20:16,440
So, let's put the integrity problem

468
00:20:16,440 --> 00:20:18,360
in perspective first, okay?

469
00:20:18,360 --> 00:20:20,080
So, 30 years ago,

470
00:20:20,080 --> 00:20:22,320
we had a huge problem with spam.

471
00:20:22,320 --> 00:20:24,040
Just e-mail spam.

472
00:20:24,040 --> 00:20:24,800
We don't really talk

473
00:20:24,800 --> 00:20:26,600
about e-mail spam anymore, right?

474
00:20:26,600 --> 00:20:28,480
So, we solve this problem.

475
00:20:28,480 --> 00:20:30,280
The hope is, that all these

476
00:20:30,280 --> 00:20:31,200
integrity violations,

477
00:20:31,200 --> 00:20:33,280
all this hate speech and misinformation,

478
00:20:33,280 --> 00:20:36,400
it will be the way of the past, like spam.

479
00:20:36,400 --> 00:20:37,800
It's a much much harder problem

480
00:20:37,800 --> 00:20:41,880
in terms of AI, and as you point out,

481
00:20:41,880 --> 00:20:43,680
the subtleties that are involved.

482
00:20:43,680 --> 00:20:47,960
But the hope is, that it'll be like spam.

483
00:20:47,960 --> 00:20:51,600
So, now the question is, I think of it,

484
00:20:51,600 --> 00:20:54,160
so, to me, integrity is,

485
00:20:54,160 --> 00:20:55,840
you come onto a social network,

486
00:20:55,840 --> 00:20:56,680
if you see something

487
00:20:56,680 --> 00:20:59,000
that violates the policies,

488
00:20:59,000 --> 00:21:01,200
that's a negative experience for you.

489
00:21:01,200 --> 00:21:01,880
Okay?

490
00:21:01,880 --> 00:21:03,160
Our goal is to minimize

491
00:21:03,160 --> 00:21:05,080
the number of negative experiences

492
00:21:05,080 --> 00:21:06,880
that you might have on social network.

493
00:21:06,880 --> 00:21:07,560
Right.

494
00:21:07,560 --> 00:21:10,400
Now, let's look at the happy side.

495
00:21:10,400 --> 00:21:12,760
How can we increase or maximize

496
00:21:12,760 --> 00:21:14,680
the number of positive experiences

497
00:21:14,680 --> 00:21:15,960
that you have, okay?

498
00:21:15,960 --> 00:21:16,720
And that's really

499
00:21:16,720 --> 00:21:18,280
where the opportunity lies.

500
00:21:18,280 --> 00:21:19,000
Okay?

501
00:21:19,000 --> 00:21:22,640
How is it, and I'm speaking here

502
00:21:22,640 --> 00:21:24,000
not authorized by Facebook,

503
00:21:24,000 --> 00:21:25,480
this is just me.

504
00:21:25,480 --> 00:21:27,680
I am asking Alon Halevy. Of course.

505
00:21:27,840 --> 00:21:29,120
How do I make it such,

506
00:21:29,120 --> 00:21:32,440
that a social network or any AI tool

507
00:21:32,440 --> 00:21:35,760
just helps me be my best self

508
00:21:35,760 --> 00:21:38,800
any given point in the day. Okay?

509
00:21:38,800 --> 00:21:41,160
So, now if I have a couple of hours free

510
00:21:41,160 --> 00:21:42,520
and I'm not sure what to do,

511
00:21:42,520 --> 00:21:45,040
the computer knows what the weather is.

512
00:21:45,040 --> 00:21:46,160
And it knows what's open.

513
00:21:46,160 --> 00:21:48,200
It knows what I've enjoyed doing

514
00:21:48,200 --> 00:21:49,720
in the past. Maybe something

515
00:21:49,720 --> 00:21:51,920
what I haven't done for a while.

516
00:21:51,920 --> 00:21:54,120
What I'm on is, an AI system

517
00:21:54,120 --> 00:21:55,520
that knows me so well,

518
00:21:55,520 --> 00:21:56,680
but it's very private, right?

519
00:21:56,680 --> 00:22:00,200
So, we're not compromising on privacy.

520
00:22:00,200 --> 00:22:01,280
I want an AI system

521
00:22:01,280 --> 00:22:03,120
that will give me advice

522
00:22:03,120 --> 00:22:05,600
at any given point in the day,

523
00:22:05,600 --> 00:22:08,520
that enable me to be the best,

524
00:22:08,520 --> 00:22:11,600
to have the best experiences that I can.

525
00:22:11,600 --> 00:22:12,720
And by the way, I think privacy

526
00:22:12,720 --> 00:22:13,680
is such an interesting issue.

527
00:22:13,680 --> 00:22:14,560
And I say that because

528
00:22:14,800 --> 00:22:17,040
I'm on the extreme end

529
00:22:17,040 --> 00:22:19,600
of I want everyone to know

530
00:22:19,600 --> 00:22:20,280
everything about me.

531
00:22:20,280 --> 00:22:21,640
I want them to know where I live,

532
00:22:21,640 --> 00:22:22,800
what I do, where I work,

533
00:22:22,800 --> 00:22:24,520
when I sleep, when I brush my teeth.

534
00:22:24,520 --> 00:22:25,280
I want to have the most

535
00:22:25,280 --> 00:22:27,400
personalized experience in the world.

536
00:22:27,400 --> 00:22:28,480
A lot of my friends,

537
00:22:28,480 --> 00:22:30,520
like my age and older,

538
00:22:30,520 --> 00:22:31,840
are very conservative.

539
00:22:31,840 --> 00:22:33,400
Like we don't want anybody to know.

540
00:22:33,400 --> 00:22:34,240
We're very scared yet.

541
00:22:34,240 --> 00:22:37,040
So, we want to enjoy personalization,

542
00:22:37,040 --> 00:22:38,720
but not at the expense of privacy.

543
00:22:38,720 --> 00:22:39,720
I'm saying to my friends

544
00:22:39,720 --> 00:22:41,360
that are younger than me now at Stanford,

545
00:22:41,360 --> 00:22:42,680
because most them are younger than me,

546
00:22:42,680 --> 00:22:44,600
and a lot of the people

547
00:22:44,600 --> 00:22:46,760
in Israel, privacy,

548
00:22:46,760 --> 00:22:48,520
I feel like the generation is like,

549
00:22:48,520 --> 00:22:52,960
I am like  the needle that's about to move

550
00:22:52,960 --> 00:22:55,360
into the realm of we want personalization.

551
00:22:55,360 --> 00:22:57,400
We understand that in order to get

552
00:22:57,400 --> 00:22:58,360
real personalization,

553
00:22:58,360 --> 00:22:59,280
we have to compromise

554
00:22:59,280 --> 00:23:00,320
a little bit of privacy

555
00:23:00,320 --> 00:23:01,720
and we're all for it.

556
00:23:01,720 --> 00:23:03,880
So, I'm a big advocate of that.

557
00:23:03,880 --> 00:23:05,360
Good. And in that case,

558
00:23:05,360 --> 00:23:06,160
I would encourage you

559
00:23:06,160 --> 00:23:07,680
to talk to your friends

560
00:23:07,680 --> 00:23:09,280
and tell them to run for office

561
00:23:09,400 --> 00:23:12,840
and be elected to very spot

562
00:23:12,960 --> 00:23:15,520
But responsibly. Because it has to be done

563
00:23:15,520 --> 00:23:16,440
responsibly, obviously.

564
00:23:16,440 --> 00:23:17,520
It needs to be done responsibly.

565
00:23:17,520 --> 00:23:20,440
And obviously fairness and biosis

566
00:23:20,440 --> 00:23:24,240
and data, because at the end of the day,

567
00:23:24,240 --> 00:23:28,000
we're susceptible to a variety of biosis

568
00:23:28,000 --> 00:23:29,000
in our audience as well.

569
00:23:29,000 --> 00:23:31,000
And this is a field that we're

570
00:23:31,000 --> 00:23:33,200
actively trying to figure out right now.

571
00:23:33,200 --> 00:23:34,480
What are you most passionate about?

572
00:23:34,480 --> 00:23:36,520
What excites you most

573
00:23:36,520 --> 00:23:40,040
in your professional career?

574
00:23:40,040 --> 00:23:42,760
Having technology help people

575
00:23:42,760 --> 00:23:44,200
in their day to day lives.

576
00:23:44,200 --> 00:23:47,040
Like you're seeing the real impact.

577
00:23:47,040 --> 00:23:49,240
Yes. So, I want something

578
00:23:49,240 --> 00:23:50,920
that actually makes the difference.

579
00:23:50,920 --> 00:23:52,880
That either reduces friction for me

580
00:23:52,880 --> 00:23:56,800
or, makes my life better in some way.

581
00:23:56,800 --> 00:24:01,040
That also, what I like to work on.

582
00:24:01,040 --> 00:24:03,560
So, but then, at the end,

583
00:24:03,560 --> 00:24:05,200
you're working within companies

584
00:24:05,200 --> 00:24:08,360
that processes takes time,

585
00:24:08,360 --> 00:24:09,320
yet when you do get

586
00:24:09,400 --> 00:24:10,240
to accomplish something,

587
00:24:10,240 --> 00:24:12,800
they were at billions of users.

588
00:24:12,800 --> 00:24:16,680
So, do you prefer working for a long time

589
00:24:16,680 --> 00:24:18,200
to get to that billion user?

590
00:24:18,200 --> 00:24:19,880
Or, do you prefer the more startup life

591
00:24:19,880 --> 00:24:21,840
where every single day

592
00:24:21,840 --> 00:24:23,720
you're making a positive impact

593
00:24:23,720 --> 00:24:26,320
on the product, on the team?

594
00:24:26,320 --> 00:24:28,800
It's not this or that.

595
00:24:28,800 --> 00:24:31,640
I think, even when you're working

596
00:24:31,640 --> 00:24:33,120
at a company like Google or Facebook,

597
00:24:33,120 --> 00:24:36,120
to get to that billion users,

598
00:24:36,120 --> 00:24:37,720
you're still working with a small team

599
00:24:37,720 --> 00:24:39,000
and you're still working with people

600
00:24:39,000 --> 00:24:41,640
and mentoring people and taking people.

601
00:24:41,640 --> 00:24:44,160
Solving hard problems every day,

602
00:24:44,160 --> 00:24:46,240
seeing people develop

603
00:24:46,240 --> 00:24:49,080
as they mature in your careers.

604
00:24:49,080 --> 00:24:51,200
So, there's a lot of that going on.

605
00:24:51,200 --> 00:24:54,360
And at the same time, publishing papers

606
00:24:54,360 --> 00:24:57,080
that have impact on academe.

607
00:24:57,080 --> 00:24:59,680
[--]

608
00:24:59,680 --> 00:25:00,800
I think without that mix,

609
00:25:00,840 --> 00:25:04,360
I would be not be very happy.

610
00:25:04,360 --> 00:25:05,080
And I think that's also

611
00:25:05,080 --> 00:25:08,640
very interesting collaboration

612
00:25:08,640 --> 00:25:09,960
between the research part of you,

613
00:25:09,960 --> 00:25:11,680
but the products part of you,

614
00:25:11,680 --> 00:25:13,320
which is not always,

615
00:25:13,320 --> 00:25:14,920
it doesn't always go hand-in-hand.

616
00:25:14,920 --> 00:25:16,840
Both, proper researcher

617
00:25:16,840 --> 00:25:18,360
or a professor in computer science,

618
00:25:18,360 --> 00:25:19,320
at the same time,

619
00:25:19,320 --> 00:25:20,600
thinking about the end user

620
00:25:20,600 --> 00:25:21,440
and how your products

621
00:25:21,440 --> 00:25:23,080
impact humanity at the end.

622
00:25:23,080 --> 00:25:23,800
Alon, before we leave,

623
00:25:23,800 --> 00:25:24,520
I want to thank you again

624
00:25:24,520 --> 00:25:25,400
for your generous time.

625
00:25:25,400 --> 00:25:26,600
Really, toda raba.

626
00:25:26,600 --> 00:25:27,480
And I have to ask you

627
00:25:27,480 --> 00:25:28,560
the most important question,

628
00:25:28,560 --> 00:25:30,120
which is three words that you would use

629
00:25:30,120 --> 00:25:32,080
to describe yourself.

630
00:25:32,080 --> 00:25:34,560
Thinks outside the box.

631
00:25:34,560 --> 00:25:37,760
Thinks outside the box.

632
00:25:37,760 --> 00:25:38,800
It sounds like that's a little bit

633
00:25:38,800 --> 00:25:39,400
of what it takes

634
00:25:39,400 --> 00:25:42,240
to be a director within Facebook AI.

635
00:25:42,240 --> 00:25:43,840
Alon, thank you very much.

636
00:25:43,840 --> 00:25:46,160
And stay safe and stay healthy.

637
00:25:46,160 --> 00:25:46,760
You, too.

638
00:25:46,760 --> 00:25:48,040
I'll see you around J-Ventures.

639
00:25:48,040 --> 00:25:49,640
I even forget to talk about that,

640
00:25:49,640 --> 00:25:50,400
but I'll see you around.

641
00:25:50,400 --> 00:25:51,680
Okay. Cool.

642
00:25:51,680 --> 00:25:53,240
Take care. Bye bye.

